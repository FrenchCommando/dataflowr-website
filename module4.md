@def sequence = ["optim-basics"]

~~~
<h1>Module 4 - Optimization for deep leaning</h1>
~~~

**Table of Contents**

\toc


## Optimization for deep leaning

{{youtube_placeholder optim-basics}}

{{yt_tsp 0 0 Recap}}
{{yt_tsp 31 0 Plan}}
{{yt_tsp 74 0 Optimization in deep learning}}
{{yt_tsp 224 0 Gradient descent variants}}
{{yt_tsp 478 0 Setting for the jupyter notebook}}
{{yt_tsp 589 0 Vanilla gradient descent}}
{{yt_tsp 734 0 Momentum}}
{{yt_tsp 938 0 Nesterov accelerated gradient descent}}
{{yt_tsp 1080 0 Adagrad}}
{{yt_tsp 1206 0 RMSProp}}
{{yt_tsp 1331 0 Adam}}
{{yt_tsp 1479 0 AMSGrad}}
{{yt_tsp 1629 0 Pytorch optimizers}}

## Slides and Notebook

- [slides](https://dataflowr.github.io/slides/module4.html)
- [notebook](https://github.com/dataflowr/notebooks/blob/master/Module4/04_gradient_descent_optimization_algorithms_empty.ipynb) in [colab](https://colab.research.google.com/github/dataflowr/notebooks/blob/master/Module4/04_gradient_descent_optimization_algorithms_empty.ipynb) An explanation of underfitting and overfitting with polynomial regression.