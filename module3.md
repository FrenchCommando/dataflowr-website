@def sequence = ["loss_functions"]

~~~
<h1>Module 3 - Loss functions for classification</h1>
~~~

**Table of Contents**

\toc


## Loss functions for classification

{{youtube_placeholder loss_functions}}

{{yt_tsp 0 0 Recap}}
{{yt_tsp 145 0 How to choose your loss?}}
{{yt_tsp 198 0 A probabilistic model for linear regression}}
{{yt_tsp 470 0 Gradient descent, learning rate, SGD}}
{{yt_tsp 690 0 Pytorch code for gradient descent}}
{{yt_tsp 915 0 A probabilistic model for logistic regression}}
{{yt_tsp 1047 0 Notations (information theory)}}
{{yt_tsp 1258 0 Likelihood for logistic regression}}
{{yt_tsp 1363 0 BCELoss}}
{{yt_tsp 1421 0 BCEWithLogitsLoss}}
{{yt_tsp 1537 0 Beware of the reduction parameter}}
{{yt_tsp 1647 0 Softmax regression}}
{{yt_tsp 1852 0 NLLLoss}}
{{yt_tsp 2088 0 Classification in pytorch}}
{{yt_tsp 2196 0 Why maximizing accuracy directly is hard?}}
{{yt_tsp 2304 0 Classification in deep learning}}
{{yt_tsp 2450 0 Regression without knowing the underlying model}}
{{yt_tsp 2578 0 Overfitting in polynomial regression}}
{{yt_tsp 2720 0 Validation set}}
{{yt_tsp 2935 0 Notion of risk and hypothesis space}}
{{yt_tsp 3280 0 estimation error and approximation error}}

## Slides and Notebook

- [slides](https://dataflowr.github.io/slides/module3.html)
- [notebook](https://github.com/dataflowr/notebooks/blob/master/Module3/03_polynomial_regression.ipynb) in [colab](https://colab.research.google.com/github/dataflowr/notebooks/blob/master/Module3/03_polynomial_regression.ipynb) An explanation of underfitting and overfitting with polynomial regression.